# DeepSeek-R1-Distill-Llama-8B æ¨¡å‹ä¸‹è½½ä¸éƒ¨ç½²å·¥å…·

è¿™æ˜¯ä¸€ä¸ªç”¨äºåœ¨Macä¸Šä¸‹è½½DeepSeek-R1-Distill-Llama-8Bæ¨¡å‹ï¼Œå¹¶åœ¨æœåŠ¡å™¨ä¸Šéƒ¨ç½²vLLMæœåŠ¡çš„å®Œæ•´å·¥å…·åŒ…ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. Macç«¯æ¨¡å‹ä¸‹è½½

#### å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
```

#### å¿«é€Ÿä¸‹è½½ï¼ˆæ¨èï¼‰
```bash
python quick_download.py
```

#### é«˜çº§ä¸‹è½½ï¼ˆæ›´å¤šé€‰é¡¹ï¼‰
```bash
python download_model.py --help
```

å¸¸ç”¨å‚æ•°ï¼š
- `--cache-dir`: æŒ‡å®šä¸‹è½½ç›®å½•ï¼ˆé»˜è®¤ï¼š./modelsï¼‰
- `--max-workers`: å¹¶å‘ä¸‹è½½çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ï¼š4ï¼‰
- `--token`: HuggingFaceè®¿é—®ä»¤ç‰Œï¼ˆå¦‚æœéœ€è¦ï¼‰

### 2. æ¨¡å‹ä¼ è¾“åˆ°æœåŠ¡å™¨

ä¸‹è½½å®Œæˆåï¼Œå°†æ¨¡å‹å‹ç¼©å¹¶ä¸Šä¼ åˆ°æœåŠ¡å™¨ï¼š

```bash
# å‹ç¼©æ¨¡å‹æ–‡ä»¶
tar -czf deepseek-model.tar.gz -C ./models .

# ä¸Šä¼ åˆ°æœåŠ¡å™¨
scp deepseek-model.tar.gz user@your-server:/path/to/models/

# åœ¨æœåŠ¡å™¨ä¸Šè§£å‹
ssh user@your-server
cd /path/to/models/
tar -xzf deepseek-model.tar.gz
```

### 3. æœåŠ¡å™¨ç«¯éƒ¨ç½²

å°†`server_deploy.py`å¤åˆ¶åˆ°æœåŠ¡å™¨ï¼Œç„¶åè¿è¡Œï¼š

```bash
# åŸºæœ¬éƒ¨ç½²
python server_deploy.py --model-path /path/to/models/deepseek-ai--DeepSeek-R1-Distill-Llama-8B

# åˆ›å»ºsystemdæœåŠ¡
python server_deploy.py --model-path /path/to/models/deepseek-ai--DeepSeek-R1-Distill-Llama-8B --create-service

# è‡ªå®šä¹‰é…ç½®
python server_deploy.py \
    --model-path /path/to/models/deepseek-ai--DeepSeek-R1-Distill-Llama-8B \
    --port 8000 \
    --tensor-parallel-size 8 \
    --max-model-len 32768
```

## ğŸ“ æ–‡ä»¶è¯´æ˜

### Macç«¯æ–‡ä»¶
- `requirements.txt`: Pythonä¾èµ–åŒ…
- `quick_download.py`: å¿«é€Ÿä¸‹è½½è„šæœ¬ï¼ˆæ¨èä½¿ç”¨ï¼‰
- `download_model.py`: å®Œæ•´åŠŸèƒ½çš„ä¸‹è½½å·¥å…·
- `README.md`: ä½¿ç”¨è¯´æ˜æ–‡æ¡£

### æœåŠ¡å™¨ç«¯æ–‡ä»¶
- `server_deploy.py`: æœåŠ¡å™¨éƒ¨ç½²è„šæœ¬
- `test_vllm.py`: æœåŠ¡æµ‹è¯•å®¢æˆ·ç«¯ï¼ˆç”±éƒ¨ç½²è„šæœ¬ç”Ÿæˆï¼‰
- `start_vllm.sh`: å¯åŠ¨è„šæœ¬ï¼ˆç”±éƒ¨ç½²è„šæœ¬ç”Ÿæˆï¼‰
- `vllm_config.json`: vLLMé…ç½®æ–‡ä»¶ï¼ˆç”±éƒ¨ç½²è„šæœ¬ç”Ÿæˆï¼‰
- `deepseek-vllm.service`: SystemdæœåŠ¡æ–‡ä»¶ï¼ˆå¯é€‰ç”Ÿæˆï¼‰

## ğŸ”§ æœåŠ¡å™¨é…ç½®è¦æ±‚

æ ¹æ®æ‚¨çš„æœåŠ¡å™¨é…ç½®ï¼š
- **GPU**: 8 Ã— NVIDIA H20 (95GBæ˜¾å­˜/å—)
- **CUDA**: 12.4
- **é©±åŠ¨**: 550.90.07
- **ç³»ç»Ÿ**: Ubuntu 22.04 LTS

æ¨èé…ç½®ï¼š
- `tensor-parallel-size`: 8 (ä½¿ç”¨æ‰€æœ‰8å—GPU)
- `gpu-memory-utilization`: 0.9 (90%æ˜¾å­˜åˆ©ç”¨ç‡)
- `max-model-len`: 32768 (æœ€å¤§åºåˆ—é•¿åº¦)

## ğŸ“Š æ¨¡å‹ä¿¡æ¯

- **æ¨¡å‹åç§°**: DeepSeek-R1-Distill-Llama-8B
- **æ¨¡å‹å¤§å°**: çº¦15-20GB
- **æ¶æ„**: Llama-based
- **ç”¨é€”**: é€šç”¨å¯¹è¯å’Œæ¨ç†

## ğŸ› ï¸ ä½¿ç”¨ç¤ºä¾‹

### å¯åŠ¨æœåŠ¡
```bash
# ä½¿ç”¨ç”Ÿæˆçš„å¯åŠ¨è„šæœ¬
./start_vllm.sh

# æˆ–è€…ç›´æ¥ä½¿ç”¨vLLMå‘½ä»¤
python -m vllm.entrypoints.openai.api_server \
    --model /path/to/model \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 8 \
    --gpu-memory-utilization 0.9
```

### æµ‹è¯•æœåŠ¡
```bash
# ä½¿ç”¨ç”Ÿæˆçš„æµ‹è¯•è„šæœ¬
python test_vllm.py

# æˆ–è€…ä½¿ç”¨curlæµ‹è¯•
curl -X POST "http://localhost:8000/v1/chat/completions" \
    -H "Content-Type: application/json" \
    -d '{
        "model": "deepseek-r1-distill-llama-8b",
        "messages": [
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
        ],
        "max_tokens": 100
    }'
```

### APIè°ƒç”¨ç¤ºä¾‹
```python
import requests

url = "http://your-server:8000/v1/chat/completions"
payload = {
    "model": "deepseek-r1-distill-llama-8b",
    "messages": [
        {"role": "user", "content": "è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†"}
    ],
    "max_tokens": 500,
    "temperature": 0.7
}

response = requests.post(url, json=payload)
result = response.json()
print(result['choices'][0]['message']['content'])
```

## ğŸ” æ•…éšœæ’é™¤

### ä¸‹è½½é—®é¢˜
1. **ç½‘ç»œè¿æ¥å¤±è´¥**: æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œè€ƒè™‘ä½¿ç”¨ä»£ç†
2. **ç£ç›˜ç©ºé—´ä¸è¶³**: ç¡®ä¿æœ‰è¶³å¤Ÿçš„å­˜å‚¨ç©ºé—´ï¼ˆè‡³å°‘30GBï¼‰
3. **æƒé™é—®é¢˜**: ç¡®ä¿å¯¹ä¸‹è½½ç›®å½•æœ‰å†™æƒé™

### éƒ¨ç½²é—®é¢˜
1. **GPUæ£€æµ‹å¤±è´¥**: æ£€æŸ¥CUDAé©±åŠ¨å’Œnvidia-smiå‘½ä»¤
2. **å†…å­˜ä¸è¶³**: è°ƒæ•´`gpu-memory-utilization`å‚æ•°
3. **ç«¯å£å ç”¨**: æ›´æ”¹ç«¯å£æˆ–åœæ­¢å ç”¨ç«¯å£çš„è¿›ç¨‹

### æœåŠ¡é—®é¢˜
1. **å¯åŠ¨å¤±è´¥**: æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œæƒé™
2. **æ¨ç†æ…¢**: è°ƒæ•´`tensor-parallel-size`å’Œ`max-model-len`
3. **å†…å­˜æº¢å‡º**: å‡å°‘`max-model-len`æˆ–`gpu-memory-utilization`

## ğŸ“ æ—¥å¿—å’Œç›‘æ§

### æŸ¥çœ‹æœåŠ¡æ—¥å¿—
```bash
# å¦‚æœä½¿ç”¨systemdæœåŠ¡
sudo journalctl -u deepseek-vllm -f

# å¦‚æœä½¿ç”¨å¯åŠ¨è„šæœ¬
tail -f vllm.log
```

### ç›‘æ§GPUä½¿ç”¨
```bash
# å®æ—¶ç›‘æ§
nvidia-smi -l 1

# æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
nvidia-smi --query-gpu=name,memory.total,memory.used,utilization.gpu --format=csv
```

## ğŸ”„ æ›´æ–°å’Œç»´æŠ¤

### æ›´æ–°æ¨¡å‹
1. ä¸‹è½½æ–°ç‰ˆæœ¬æ¨¡å‹
2. åœæ­¢æœåŠ¡
3. æ›¿æ¢æ¨¡å‹æ–‡ä»¶
4. é‡å¯æœåŠ¡

### æ›´æ–°vLLM
```bash
pip install --upgrade vllm
```

## ğŸ“ æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. ç³»ç»Ÿè¦æ±‚æ˜¯å¦æ»¡è¶³
2. ä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…
3. æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´
4. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸

## ğŸ“„ è®¸å¯è¯

æœ¬å·¥å…·éµå¾ªMITè®¸å¯è¯ã€‚æ¨¡å‹ä½¿ç”¨è¯·éµå¾ªDeepSeekçš„è®¸å¯è¯æ¡æ¬¾ã€‚ 