#!/usr/bin/env python3
"""
æœåŠ¡å™¨ç«¯éƒ¨ç½²è„šæœ¬
ç”¨äºåœ¨æœåŠ¡å™¨ä¸Šå¯åŠ¨ DeepSeek-R1-Distill-Llama-8B æ¨¡å‹æœåŠ¡
"""

import os
import sys
import argparse
import subprocess
import json
from pathlib import Path
import psutil
import GPUtil

def check_system_requirements():
    """æ£€æŸ¥ç³»ç»Ÿè¦æ±‚"""
    print("ğŸ” æ£€æŸ¥ç³»ç»Ÿè¦æ±‚...")
    
    # æ£€æŸ¥GPU
    try:
        gpus = GPUtil.getGPUs()
        if not gpus:
            print("âŒ æœªæ£€æµ‹åˆ°GPU")
            return False
        
        print(f"âœ… æ£€æµ‹åˆ° {len(gpus)} ä¸ªGPU:")
        for i, gpu in enumerate(gpus):
            print(f"   GPU {i}: {gpu.name} ({gpu.memoryTotal}MB)")
    except:
        print("âš ï¸  æ— æ³•æ£€æµ‹GPUä¿¡æ¯ï¼Œè¯·ç¡®ä¿å®‰è£…äº†nvidia-ml-py")
    
    # æ£€æŸ¥å†…å­˜
    memory = psutil.virtual_memory()
    print(f"ğŸ’¾ ç³»ç»Ÿå†…å­˜: {memory.total / (1024**3):.1f} GB")
    
    # æ£€æŸ¥CUDA
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… CUDAç¯å¢ƒæ­£å¸¸")
        else:
            print("âŒ CUDAç¯å¢ƒå¼‚å¸¸")
            return False
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°nvidia-smiå‘½ä»¤")
        return False
    
    return True

def install_dependencies():
    """å®‰è£…ä¾èµ–"""
    print("ğŸ“¦ å®‰è£…ä¾èµ–...")
    
    dependencies = [
        "vllm",
        "transformers",
        "torch",
        "fastapi",
        "uvicorn"
    ]
    
    for dep in dependencies:
        try:
            print(f"å®‰è£… {dep}...")
            subprocess.run([sys.executable, "-m", "pip", "install", dep], 
                         check=True, capture_output=True)
            print(f"âœ… {dep} å®‰è£…æˆåŠŸ")
        except subprocess.CalledProcessError as e:
            print(f"âŒ {dep} å®‰è£…å¤±è´¥: {e}")
            return False
    
    return True

def create_vllm_config(model_path: str, port: int = 8000, 
                      tensor_parallel_size: int = 8,
                      max_model_len: int = 32768) -> str:
    """åˆ›å»ºvLLMé…ç½®æ–‡ä»¶"""
    config = {
        "model": model_path,
        "host": "0.0.0.0",
        "port": port,
        "tensor_parallel_size": tensor_parallel_size,
        "max_model_len": max_model_len,
        "gpu_memory_utilization": 0.9,
        "trust_remote_code": True,
        "disable_log_stats": False,
        "served_model_name": "deepseek-r1-distill-llama-8b"
    }
    
    config_file = "vllm_config.json"
    with open(config_file, 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"âœ… é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_file}")
    return config_file

def create_startup_script(model_path: str, port: int = 8000,
                         tensor_parallel_size: int = 8,
                         max_model_len: int = 32768) -> str:
    """åˆ›å»ºå¯åŠ¨è„šæœ¬"""
    script_content = f"""#!/bin/bash
# DeepSeek-R1-Distill-Llama-8B vLLM å¯åŠ¨è„šæœ¬

echo "ğŸš€ å¯åŠ¨ DeepSeek-R1-Distill-Llama-8B æœåŠ¡..."
echo "æ¨¡å‹è·¯å¾„: {model_path}"
echo "ç«¯å£: {port}"
echo "GPUæ•°é‡: {tensor_parallel_size}"
echo "æœ€å¤§åºåˆ—é•¿åº¦: {max_model_len}"
echo "=" * 60

# è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export VLLM_USE_MODELSCOPE=False

# å¯åŠ¨vLLMæœåŠ¡
python -m vllm.entrypoints.openai.api_server \\
    --model {model_path} \\
    --host 0.0.0.0 \\
    --port {port} \\
    --tensor-parallel-size {tensor_parallel_size} \\
    --max-model-len {max_model_len} \\
    --gpu-memory-utilization 0.9 \\
    --trust-remote-code \\
    --served-model-name deepseek-r1-distill-llama-8b \\
    --disable-log-stats
"""
    
    script_file = "start_vllm.sh"
    with open(script_file, 'w') as f:
        f.write(script_content)
    
    # æ·»åŠ æ‰§è¡Œæƒé™
    os.chmod(script_file, 0o755)
    
    print(f"âœ… å¯åŠ¨è„šæœ¬å·²åˆ›å»º: {script_file}")
    return script_file

def create_systemd_service(model_path: str, working_dir: str,
                          port: int = 8000, tensor_parallel_size: int = 8) -> str:
    """åˆ›å»ºsystemdæœåŠ¡æ–‡ä»¶"""
    service_content = f"""[Unit]
Description=DeepSeek-R1-Distill-Llama-8B vLLM Service
After=network.target

[Service]
Type=simple
User=root
WorkingDirectory={working_dir}
Environment=CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
Environment=VLLM_USE_MODELSCOPE=False
ExecStart=/usr/bin/python3 -m vllm.entrypoints.openai.api_server \\
    --model {model_path} \\
    --host 0.0.0.0 \\
    --port {port} \\
    --tensor-parallel-size {tensor_parallel_size} \\
    --max-model-len 32768 \\
    --gpu-memory-utilization 0.9 \\
    --trust-remote-code \\
    --served-model-name deepseek-r1-distill-llama-8b
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
"""
    
    service_file = "deepseek-vllm.service"
    with open(service_file, 'w') as f:
        f.write(service_content)
    
    print(f"âœ… SystemdæœåŠ¡æ–‡ä»¶å·²åˆ›å»º: {service_file}")
    print("ğŸ“ è¦å®‰è£…æœåŠ¡ï¼Œè¯·è¿è¡Œ:")
    print(f"   sudo cp {service_file} /etc/systemd/system/")
    print("   sudo systemctl daemon-reload")
    print("   sudo systemctl enable deepseek-vllm")
    print("   sudo systemctl start deepseek-vllm")
    
    return service_file

def create_test_client() -> str:
    """åˆ›å»ºæµ‹è¯•å®¢æˆ·ç«¯"""
    test_content = '''#!/usr/bin/env python3
"""
vLLMæœåŠ¡æµ‹è¯•å®¢æˆ·ç«¯
"""

import requests
import json

def test_vllm_service(host="localhost", port=8000):
    """æµ‹è¯•vLLMæœåŠ¡"""
    base_url = f"http://{host}:{port}"
    
    # æµ‹è¯•å¥åº·æ£€æŸ¥
    try:
        response = requests.get(f"{base_url}/health")
        if response.status_code == 200:
            print("âœ… æœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡")
        else:
            print(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡: {e}")
        return False
    
    # æµ‹è¯•æ¨¡å‹åˆ—è¡¨
    try:
        response = requests.get(f"{base_url}/v1/models")
        if response.status_code == 200:
            models = response.json()
            print(f"âœ… å¯ç”¨æ¨¡å‹: {[m['id'] for m in models['data']]}")
        else:
            print(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {response.status_code}")
    except Exception as e:
        print(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¼‚å¸¸: {e}")
    
    # æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
    try:
        payload = {
            "model": "deepseek-r1-distill-llama-8b",
            "messages": [
                {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
            ],
            "max_tokens": 100,
            "temperature": 0.7
        }
        
        print("ğŸ§ª æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
        response = requests.post(f"{base_url}/v1/chat/completions", 
                               json=payload, timeout=30)
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            print(f"âœ… ç”ŸæˆæˆåŠŸ:")
            print(f"   {content}")
            return True
        else:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {response.status_code}")
            print(f"   {response.text}")
            return False
            
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¼‚å¸¸: {e}")
        return False

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æµ‹è¯•vLLMæœåŠ¡")
    parser.add_argument("--host", default="localhost", help="æœåŠ¡å™¨åœ°å€")
    parser.add_argument("--port", type=int, default=8000, help="æœåŠ¡ç«¯å£")
    
    args = parser.parse_args()
    
    print(f"ğŸ” æµ‹è¯•vLLMæœåŠ¡ {args.host}:{args.port}")
    print("=" * 50)
    
    success = test_vllm_service(args.host, args.port)
    
    if success:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡çŠ¶æ€")
'''
    
    test_file = "test_vllm.py"
    with open(test_file, 'w') as f:
        f.write(test_content)
    
    os.chmod(test_file, 0o755)
    
    print(f"âœ… æµ‹è¯•å®¢æˆ·ç«¯å·²åˆ›å»º: {test_file}")
    return test_file

def main():
    parser = argparse.ArgumentParser(description="DeepSeek-R1-Distill-Llama-8B æœåŠ¡å™¨éƒ¨ç½²å·¥å…·")
    parser.add_argument("--model-path", required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--port", type=int, default=8000, help="æœåŠ¡ç«¯å£")
    parser.add_argument("--tensor-parallel-size", type=int, default=8, help="GPUå¹¶è¡Œæ•°é‡")
    parser.add_argument("--max-model-len", type=int, default=32768, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--skip-install", action="store_true", help="è·³è¿‡ä¾èµ–å®‰è£…")
    parser.add_argument("--create-service", action="store_true", help="åˆ›å»ºsystemdæœåŠ¡")
    
    args = parser.parse_args()
    
    print("ğŸš€ DeepSeek-R1-Distill-Llama-8B æœåŠ¡å™¨éƒ¨ç½²")
    print("=" * 60)
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not Path(args.model_path).exists():
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
        sys.exit(1)
    
    # æ£€æŸ¥ç³»ç»Ÿè¦æ±‚
    if not check_system_requirements():
        print("âŒ ç³»ç»Ÿè¦æ±‚æ£€æŸ¥å¤±è´¥")
        sys.exit(1)
    
    # å®‰è£…ä¾èµ–
    if not args.skip_install:
        if not install_dependencies():
            print("âŒ ä¾èµ–å®‰è£…å¤±è´¥")
            sys.exit(1)
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶
    config_file = create_vllm_config(
        args.model_path, 
        args.port, 
        args.tensor_parallel_size,
        args.max_model_len
    )
    
    # åˆ›å»ºå¯åŠ¨è„šæœ¬
    startup_script = create_startup_script(
        args.model_path,
        args.port,
        args.tensor_parallel_size,
        args.max_model_len
    )
    
    # åˆ›å»ºæµ‹è¯•å®¢æˆ·ç«¯
    test_client = create_test_client()
    
    # åˆ›å»ºsystemdæœåŠ¡ï¼ˆå¯é€‰ï¼‰
    if args.create_service:
        service_file = create_systemd_service(
            args.model_path,
            os.getcwd(),
            args.port,
            args.tensor_parallel_size
        )
    
    print("\nğŸ‰ éƒ¨ç½²å‡†å¤‡å®Œæˆï¼")
    print("=" * 60)
    print("ğŸ“ ä¸‹ä¸€æ­¥æ“ä½œ:")
    print(f"1. å¯åŠ¨æœåŠ¡: ./{startup_script}")
    print(f"2. æµ‹è¯•æœåŠ¡: python {test_client}")
    print("3. æŸ¥çœ‹æ—¥å¿—: tail -f vllm.log")
    
    if args.create_service:
        print("\nğŸ”§ SystemdæœåŠ¡:")
        print("1. å®‰è£…æœåŠ¡: sudo cp deepseek-vllm.service /etc/systemd/system/")
        print("2. é‡è½½é…ç½®: sudo systemctl daemon-reload")
        print("3. å¯ç”¨æœåŠ¡: sudo systemctl enable deepseek-vllm")
        print("4. å¯åŠ¨æœåŠ¡: sudo systemctl start deepseek-vllm")
        print("5. æŸ¥çœ‹çŠ¶æ€: sudo systemctl status deepseek-vllm")

if __name__ == "__main__":
    main() 