#!/bin/bash

# DeepSeek-R1-Distill-Llama-8B H20*8æœåŠ¡å™¨å®‰è£…è„šæœ¬
# æ”¯æŒUbuntu 22.04 LTS + CUDA 12.4 + H20*8 GPU

set -e

echo "=========================================="
echo "DeepSeek-R1 vLLM H20*8 æœåŠ¡å™¨å®‰è£…è„šæœ¬"
echo "=========================================="

# # æ£€æŸ¥æ˜¯å¦ä¸ºrootç”¨æˆ·
# if [[ $EUID -eq 0 ]]; then
#    echo "è¯·ä¸è¦ä½¿ç”¨rootç”¨æˆ·è¿è¡Œæ­¤è„šæœ¬"
#    exit 1
# fi

# æ£€æŸ¥CUDAç‰ˆæœ¬
echo "æ£€æŸ¥CUDAç¯å¢ƒ..."
if ! command -v nvidia-smi &> /dev/null; then
    echo "âŒ æœªæ£€æµ‹åˆ°NVIDIAé©±åŠ¨ï¼Œè¯·å…ˆå®‰è£…NVIDIAé©±åŠ¨"
    exit 1
fi

nvidia-smi
echo "âœ… NVIDIAé©±åŠ¨æ£€æµ‹æ­£å¸¸"

# æ£€æŸ¥GPUæ•°é‡
GPU_COUNT=$(nvidia-smi --list-gpus | wc -l)
echo "æ£€æµ‹åˆ° $GPU_COUNT å—GPU"

if [ $GPU_COUNT -lt 8 ]; then
    echo "âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°GPUæ•°é‡å°‘äº8å—ï¼Œå°†è°ƒæ•´tensor_parallel_size"
fi

# æ›´æ–°ç³»ç»ŸåŒ…
echo "æ›´æ–°ç³»ç»ŸåŒ…..."
sudo apt update && sudo apt upgrade -y

# å®‰è£…ç³»ç»Ÿä¾èµ–
echo "å®‰è£…ç³»ç»Ÿä¾èµ–..."
sudo apt install -y \
    python3.10 \
    python3.10-dev \
    python3.10-venv \
    python3-pip \
    build-essential \
    git \
    wget \
    curl \
    htop \
    nvtop \
    vim \
    tmux \
    screen

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆå¯é€‰ï¼‰
read -p "æ˜¯å¦åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Ÿ(y/n): " create_venv
if [[ $create_venv == "y" || $create_venv == "Y" ]]; then
    echo "ğŸ“¦ åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ..."
    python3 -m venv deepseek_env
    source deepseek_env/bin/activate
    echo "âœ… è™šæ‹Ÿç¯å¢ƒå·²åˆ›å»ºå¹¶æ¿€æ´»"
    echo "ğŸ’¡ ä¸‹æ¬¡ä½¿ç”¨å‰è¯·è¿è¡Œ: source deepseek_env/bin/activate"
fi

# å‡çº§pip
echo "å‡çº§pip..."
pip install --upgrade pip setuptools wheel

# å®‰è£…PyTorch (CUDA 12.1ç‰ˆæœ¬å…¼å®¹CUDA 12.4)
echo "å®‰è£…PyTorch with CUDAæ”¯æŒ..."
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# éªŒè¯PyTorch CUDAæ”¯æŒ
echo "éªŒè¯PyTorch CUDAæ”¯æŒ..."
python -c "
import torch
print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')
print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')
print(f'CUDAç‰ˆæœ¬: {torch.version.cuda}')
print(f'GPUæ•°é‡: {torch.cuda.device_count()}')
if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
"

# å®‰è£…vLLM
echo "å®‰è£…vLLM..."
pip install vllm

# å®‰è£…å…¶ä»–ä¾èµ–
echo "å®‰è£…é¡¹ç›®ä¾èµ–..."
pip install -r requirements.txt

# å®‰è£…Flash Attention 2 (å¯é€‰ï¼Œæ€§èƒ½ä¼˜åŒ–)
# echo "å®‰è£…Flash Attention 2..."
# pip install flash-attn --no-build-isolation

# åˆ›å»ºæ¨¡å‹ç¼“å­˜ç›®å½•
echo "åˆ›å»ºæ¨¡å‹ç¼“å­˜ç›®å½•..."
mkdir -p model_cache logs

# è®¾ç½®ç¯å¢ƒå˜é‡
echo "é…ç½®ç¯å¢ƒå˜é‡..."
cat >> ~/.bashrc << EOF

# DeepSeek-R1 vLLM ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export TRANSFORMERS_CACHE=$(pwd)/model_cache
export HF_HOME=$(pwd)/model_cache
export HF_HUB_ENABLE_HF_TRANSFER=1
export VLLM_USE_MODELSCOPE=false
export HF_ENDPOINT=https://hf-mirror.com
EOF

# é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡
source ~/.bashrc

# é¢„ä¸‹è½½æ¨¡å‹ (å¯é€‰)
echo "æ˜¯å¦è¦é¢„ä¸‹è½½æ¨¡å‹? (y/n)"
read -r download_model
if [[ $download_model == "y" || $download_model == "Y" ]]; then
    echo "å¼€å§‹ä¸‹è½½DeepSeek-R1-Distill-Llama-8Bæ¨¡å‹..."
    export TRANSFORMERS_CACHE=$(pwd)/model_cache
    export HF_HOME=$(pwd)/model_cache
    export HF_HUB_ENABLE_HF_TRANSFER=1
    # ç¡®ä¿ä½¿ç”¨åœ¨çº¿æ¨¡å¼
    export TRANSFORMERS_OFFLINE=0
    unset HF_HUB_OFFLINE
    # ä½¿ç”¨é•œåƒ
    export HF_ENDPOINT=https://hf-mirror.com
    
    python -c "
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

print('å¼€å§‹ä¸‹è½½æ¨¡å‹...')
model_name = 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B'

# ä¸‹è½½tokenizer
print('ä¸‹è½½tokenizer...')
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True,
    cache_dir='./model_cache'
)

print('ä¸‹è½½å®Œæˆ!')
print(f'æ¨¡å‹ç¼“å­˜ä½ç½®: {model_name}')
"
fi

# åˆ›å»ºç³»ç»ŸæœåŠ¡æ–‡ä»¶ (å¯é€‰)
echo "æ˜¯å¦åˆ›å»ºsystemdæœåŠ¡? (y/n)"
read -r create_service
if [[ $create_service == "y" || $create_service == "Y" ]]; then
    sudo tee /etc/systemd/system/deepseek-web3.service > /dev/null << EOF
[Unit]
Description=DeepSeek-R1 vLLM Web3 Quant API Server
After=network.target

[Service]
Type=simple
User=$USER
WorkingDirectory=$(pwd)
Environment=PATH=$(pwd)/venv/bin
Environment=CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
Environment=TRANSFORMERS_CACHE=$(pwd)/model_cache
Environment=HF_HOME=$(pwd)/model_cache
Environment=HF_HUB_ENABLE_HF_TRANSFER=1
ExecStart=$(pwd)/venv/bin/python DeepSeek-R1-Distill-Llama-8B-app.py --host 0.0.0.0 --port 8000
Restart=always
RestartSec=3
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
EOF

    sudo systemctl daemon-reload
    echo "âœ… systemdæœåŠ¡å·²åˆ›å»ºï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç®¡ç†:"
    echo "å¯åŠ¨æœåŠ¡: sudo systemctl start deepseek-web3"
    echo "å¼€æœºè‡ªå¯: sudo systemctl enable deepseek-web3"
    echo "æŸ¥çœ‹çŠ¶æ€: sudo systemctl status deepseek-web3"
    echo "æŸ¥çœ‹æ—¥å¿—: sudo journalctl -u deepseek-web3 -f"
fi

# åˆ›å»ºå¯åŠ¨è„šæœ¬
echo "åˆ›å»ºå¯åŠ¨è„šæœ¬..."
cat > start_server.sh << 'EOF'
#!/bin/bash

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export TRANSFORMERS_CACHE=$(pwd)/model_cache
export HF_HOME=$(pwd)/model_cache
export HF_HUB_ENABLE_HF_TRANSFER=1

# æ£€æŸ¥GPUçŠ¶æ€
echo "æ£€æŸ¥GPUçŠ¶æ€..."
nvidia-smi

echo "å¯åŠ¨DeepSeek-R1 vLLMæœåŠ¡å™¨..."
python DeepSeek-R1-Distill-Llama-8B-app.py --host 0.0.0.0 --port 8000
EOF

chmod +x start_server.sh

# åˆ›å»ºæµ‹è¯•è„šæœ¬
cat > test_api.py << 'EOF'
#!/usr/bin/env python3
"""
APIæµ‹è¯•è„šæœ¬
"""

import requests
import json
import time

def test_health():
    """æµ‹è¯•å¥åº·æ£€æŸ¥æ¥å£"""
    try:
        response = requests.get("http://localhost:8000/health", timeout=10)
        print("å¥åº·æ£€æŸ¥:", response.json())
        return response.status_code == 200
    except Exception as e:
        print(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return False

def test_chat():
    """æµ‹è¯•èŠå¤©æ¥å£"""
    try:
        data = {
            "messages": [
                {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
            ],
            "max_tokens": 100,
            "temperature": 0.7
        }
        
        response = requests.post(
            "http://localhost:8000/chat",
            json=data,
            timeout=60
        )
        
        result = response.json()
        print("èŠå¤©æµ‹è¯•:")
        print(f"å“åº”: {result.get('response', '')}")
        return response.status_code == 200
    except Exception as e:
        print(f"èŠå¤©æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_quant_strategy():
    """æµ‹è¯•é‡åŒ–ç­–ç•¥ç”Ÿæˆ"""
    try:
        data = {
            "market_data": "BTCä»·æ ¼åœ¨65000-67000åŒºé—´éœ‡è¡ï¼Œæˆäº¤é‡è¾ƒå‰æ—¥å¢åŠ 15%",
            "strategy_type": "ç½‘æ ¼äº¤æ˜“",
            "risk_level": "ä¸­",
            "timeframe": "15m",
            "target_asset": "BTC/USDT",
            "capital": 10000
        }
        
        response = requests.post(
            "http://localhost:8000/generate-quant-strategy",
            json=data,
            timeout=120
        )
        
        result = response.json()
        print("é‡åŒ–ç­–ç•¥æµ‹è¯•:")
        print(f"ç­–ç•¥ID: {result.get('strategy_id', '')}")
        print(f"ç­–ç•¥é¢„è§ˆ: {result.get('strategy', '')[:200]}...")
        return response.status_code == 200
    except Exception as e:
        print(f"é‡åŒ–ç­–ç•¥æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("å¼€å§‹APIæµ‹è¯•...")
    
    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    print("ç­‰å¾…æœåŠ¡å¯åŠ¨...")
    for i in range(30):
        if test_health():
            print("âœ… æœåŠ¡å·²å¯åŠ¨")
            break
        time.sleep(2)
        print(f"ç­‰å¾…ä¸­... ({i+1}/30)")
    else:
        print("âŒ æœåŠ¡å¯åŠ¨è¶…æ—¶")
        exit(1)
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        ("èŠå¤©æ¥å£", test_chat),
        ("é‡åŒ–ç­–ç•¥", test_quant_strategy),
    ]
    
    for name, test_func in tests:
        print(f"\næµ‹è¯• {name}...")
        if test_func():
            print(f"âœ… {name} æµ‹è¯•é€šè¿‡")
        else:
            print(f"âŒ {name} æµ‹è¯•å¤±è´¥")
EOF

chmod +x test_api.py

echo "=========================================="
echo "âœ… å®‰è£…å®Œæˆ!"
echo "=========================================="
echo ""
echo "ä½¿ç”¨æ–¹æ³•:"
echo "1. å¯åŠ¨æœåŠ¡: ./start_server.sh"
echo "2. æµ‹è¯•API: python test_api.py"
echo "3. æœåŠ¡åœ°å€: http://localhost:8000"
echo "4. APIæ–‡æ¡£: http://localhost:8000/docs"
echo ""
echo "ä¸»è¦APIç«¯ç‚¹:"
echo "- /health - å¥åº·æ£€æŸ¥"
echo "- /chat - èŠå¤©å¯¹è¯"
echo "- /generate-quant-strategy - ç”Ÿæˆé‡åŒ–ç­–ç•¥"
echo "- /analyze-market - å¸‚åœºåˆ†æ"
echo "- /optimize-strategy - ç­–ç•¥ä¼˜åŒ–"
echo ""
echo "å¦‚æœå®‰è£…äº†systemdæœåŠ¡ï¼Œå¯ä»¥ä½¿ç”¨:"
echo "- sudo systemctl start deepseek-web3"
echo "- sudo systemctl status deepseek-web3"
echo ""
echo "æ³¨æ„äº‹é¡¹:"
echo "- é¦–æ¬¡å¯åŠ¨ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œéœ€è¦ä¸€äº›æ—¶é—´"
echo "- ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´å­˜å‚¨æ¨¡å‹(çº¦15GB)"
echo "- æ¨¡å‹åŠ è½½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…"
echo "" 