# DeepSeek-R1-Distill-Llama-8B H20*8 æœåŠ¡å™¨éƒ¨ç½²æŒ‡å—

```bash
cd /project/deepseek-web3/

conda activate deepseek_env

cd vllm

#å¯åŠ¨æœåŠ¡
./start_server.sh

# æµ‹è¯•
python test_api.py
```


## ğŸ–¥ï¸ æœåŠ¡å™¨ç¯å¢ƒ
- **GPU**: 8 Ã— NVIDIA H20 (æ¯å— ~95GB æ˜¾å­˜)
- **CUDA**: 12.4
- **é©±åŠ¨**: 550.90.07
- **æ“ä½œç³»ç»Ÿ**: Ubuntu 22.04 LTS
- **æ€»æ˜¾å­˜**: ~760GB (8 Ã— 95GB)

## ğŸš€ å¿«é€Ÿéƒ¨ç½²

### 1. ç¯å¢ƒæ£€æŸ¥
```bash
# æ£€æŸ¥CUDAå’ŒGPUçŠ¶æ€
nvidia-smi

# æ£€æŸ¥CUDAç‰ˆæœ¬
nvcc --version

# æ£€æŸ¥é©±åŠ¨ç‰ˆæœ¬
cat /proc/driver/nvidia/version
```

### 2. ä¸€é”®å®‰è£…
```bash
# å…‹éš†é¡¹ç›®åˆ°vllmç›®å½•
cd vllm

# ç»™å®‰è£…è„šæœ¬æ‰§è¡Œæƒé™
chmod +x install.sh

# è¿è¡Œè‡ªåŠ¨åŒ–å®‰è£…è„šæœ¬
./install.sh
```

å®‰è£…è„šæœ¬ä¼šè‡ªåŠ¨å®Œæˆï¼š
- âœ… ç³»ç»Ÿä¾èµ–å®‰è£…
- âœ… Python 3.10 è™šæ‹Ÿç¯å¢ƒåˆ›å»º
- âœ… PyTorch (CUDA 12.1) å®‰è£…
- âœ… vLLM æ¡†æ¶å®‰è£…
- âœ… é¡¹ç›®ä¾èµ–å®‰è£…
- âœ… Flash Attention 2 ä¼˜åŒ–
- âœ… ç¯å¢ƒå˜é‡é…ç½®
- âœ… æ¨¡å‹é¢„ä¸‹è½½ (å¯é€‰)
- âœ… systemd æœåŠ¡åˆ›å»º (å¯é€‰)

### 3. å¯åŠ¨æœåŠ¡
```bash
# æ–¹å¼1: ç›´æ¥å¯åŠ¨
./start_server.sh

# æ–¹å¼2: ä½¿ç”¨systemdæœåŠ¡ (å¦‚æœå·²åˆ›å»º)
sudo systemctl start deepseek-vllm
sudo systemctl enable deepseek-vllm  # å¼€æœºè‡ªå¯

# æ–¹å¼3: åå°è¿è¡Œ
nohup ./start_server.sh > server.log 2>&1 &

# æ–¹å¼4: ä½¿ç”¨screen/tmux
screen -S deepseek
./start_server.sh
# Ctrl+A+D åˆ†ç¦»ä¼šè¯
```

### 4. éªŒè¯éƒ¨ç½²
```bash
# æµ‹è¯•APIåŠŸèƒ½
python test_api.py

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:8000/health

# æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µ
watch -n 1 nvidia-smi
```

## ğŸ“Š æ€§èƒ½é…ç½®

### H20*8 ä¼˜åŒ–é…ç½®

å½“å‰é…ç½®å·²é’ˆå¯¹ä½ çš„H20*8æœåŠ¡å™¨ä¼˜åŒ–ï¼š

```python
class VLLMConfig:
    def __init__(self):
        self.model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"  # 8Bæ¨¡å‹
        self.tensor_parallel_size = 8           # ä½¿ç”¨å…¨éƒ¨8å¼ H20
        self.gpu_memory_utilization = 0.85      # H20æ˜¾å­˜åˆ©ç”¨ç‡85%
        self.max_model_len = 32768             # 32Kä¸Šä¸‹æ–‡é•¿åº¦
        self.dtype = "bfloat16"                # bfloat16ç²¾åº¦
        self.quantization = None               # æ˜¾å­˜å……è¶³ï¼Œæ— éœ€é‡åŒ–
        self.max_num_seqs = 256                # æ”¯æŒ256å¹¶å‘è¯·æ±‚
        self.max_num_batched_tokens = 8192     # æ‰¹å¤„ç†tokens
```

### å¯åŠ¨å‚æ•°
```bash
python DeepSeek-R1-Distill-Llama-8B-app.py \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 8 \
    --gpu-memory-utilization 0.85 \
    --model-name deepseek-ai/DeepSeek-R1-Distill-Llama-8B
```

## ğŸ”§ API ä½¿ç”¨æŒ‡å—

### å¥åº·æ£€æŸ¥
```bash
curl http://localhost:8000/health
```

### èŠå¤©å¯¹è¯
```bash
curl -X POST "http://localhost:8000/chat" \
-H "Content-Type: application/json" \
-d '{
  "messages": [
    {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
  ],
  "max_tokens": 100,
  "temperature": 0.7
}'
```

### ç”Ÿæˆé‡åŒ–ç­–ç•¥
```bash
curl -X POST "http://localhost:8000/generate-quant-strategy" \
-H "Content-Type: application/json" \
-d '{
  "market_data": "BTCä»·æ ¼åœ¨65000-67000åŒºé—´éœ‡è¡ï¼Œæˆäº¤é‡è¾ƒå‰æ—¥å¢åŠ 15%",
  "strategy_type": "ç½‘æ ¼äº¤æ˜“",
  "risk_level": "ä¸­",
  "timeframe": "15m",
  "target_asset": "BTC/USDT",
  "capital": 10000
}'
```

### å¸‚åœºåˆ†æ
```bash
curl -X POST "http://localhost:8000/analyze-market" \
-H "Content-Type: application/json" \
-d '{
  "symbol": "BTC/USDT",
  "timeframe": "1h",
  "indicators": ["RSI", "MACD", "Bollinger"]
}'
```

## ğŸ“ˆ ç›‘æ§å’Œæ—¥å¿—

### ç³»ç»Ÿç›‘æ§
```bash
# GPUä½¿ç”¨æƒ…å†µ
nvidia-smi -l 1

# æ›´è¯¦ç»†çš„GPUç›‘æ§
nvtop

# ç³»ç»Ÿèµ„æº
htop

# ç£ç›˜ä½¿ç”¨
df -h
```

### åº”ç”¨æ—¥å¿—
```bash
# æŸ¥çœ‹åº”ç”¨æ—¥å¿—
tail -f logs/vllm_app_*.log

# systemdæœåŠ¡æ—¥å¿—
sudo journalctl -u deepseek-vllm -f

# å®æ—¶ç›‘æ§æ—¥å¿—
tail -f server.log
```

### PrometheusæŒ‡æ ‡
```bash
# è·å–ç›‘æ§æŒ‡æ ‡
curl http://localhost:8000/metrics
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. æ¨¡å‹åŠ è½½å¤±è´¥
```bash
# æ£€æŸ¥æ¨¡å‹ç¼“å­˜
ls -la model_cache/

# é‡æ–°ä¸‹è½½æ¨¡å‹
rm -rf model_cache/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B
python -c "
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(
    'deepseek-ai/DeepSeek-R1-Distill-Llama-8B',
    cache_dir='./model_cache',
    trust_remote_code=True
)
"
```

#### 2. GPUå†…å­˜ä¸è¶³
```bash
# é™ä½æ˜¾å­˜åˆ©ç”¨ç‡
python DeepSeek-R1-Distill-Llama-8B-app.py --gpu-memory-utilization 0.7

# å‡å°‘å¹¶å‘è¯·æ±‚æ•°
# ä¿®æ”¹VLLMConfigä¸­çš„max_num_seqs = 128
```

#### 3. æœåŠ¡å¯åŠ¨æ…¢
é¦–æ¬¡å¯åŠ¨ä¼šä¸‹è½½æ¨¡å‹ï¼Œå¤§çº¦éœ€è¦ï¼š
- **æ¨¡å‹ä¸‹è½½**: 15GBï¼Œçº¦10-30åˆ†é’Ÿ (å–å†³äºç½‘ç»œ)
- **æ¨¡å‹åŠ è½½**: çº¦2-5åˆ†é’Ÿ
- **å¼•æ“åˆå§‹åŒ–**: çº¦1-2åˆ†é’Ÿ

#### 4. APIå“åº”æ…¢
```bash
# æ£€æŸ¥GPUä½¿ç”¨ç‡
nvidia-smi

# è°ƒæ•´æ¸©åº¦å‚æ•°é™ä½æ¨ç†æ—¶é—´
curl -X POST "http://localhost:8000/chat" \
-H "Content-Type: application/json" \
-d '{
  "messages": [{"role": "user", "content": "ä½ å¥½"}],
  "temperature": 0.1,
  "max_tokens": 50
}'
```

## ğŸ”§ é«˜çº§é…ç½®

### ç¯å¢ƒå˜é‡
```bash
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export TRANSFORMERS_CACHE=/path/to/model_cache
export HF_HOME=/path/to/model_cache
export HF_HUB_ENABLE_HF_TRANSFER=1
export VLLM_USE_MODELSCOPE=false
```

### è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„
```bash
# å¦‚æœä½¿ç”¨æœ¬åœ°æ¨¡å‹
python DeepSeek-R1-Distill-Llama-8B-app.py \
    --model-name /path/to/local/model
```

### è°ƒæ•´å¹¶å‘é…ç½®
ä¿®æ”¹ `VLLMConfig` ç±»ä¸­çš„å‚æ•°ï¼š
```python
self.max_num_seqs = 128          # å‡å°‘å¹¶å‘é™ä½æ˜¾å­˜ä½¿ç”¨
self.max_num_batched_tokens = 4096  # å‡å°‘æ‰¹å¤„ç†å¤§å°
```

## ğŸ“Š æ€§èƒ½é¢„æœŸ

### H20*8 é…ç½®ä¸‹çš„é¢„æœŸæ€§èƒ½

- **æ¨¡å‹å¤§å°**: ~15GB
- **æ˜¾å­˜å ç”¨**: çº¦35-45GB (æ¯å¼ H20)
- **æ¨ç†é€Ÿåº¦**: ~50-100 tokens/s
- **å¹¶å‘èƒ½åŠ›**: 100+ å¹¶å‘è¯·æ±‚
- **ä¸Šä¸‹æ–‡é•¿åº¦**: 32K tokens
- **å¯åŠ¨æ—¶é—´**: 3-8åˆ†é’Ÿ (é¦–æ¬¡)

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **æ˜¾å­˜ä¼˜åŒ–**:
   - ä½¿ç”¨ `bfloat16` ç²¾åº¦
   - é€‚å½“è°ƒæ•´ `gpu_memory_utilization`
   - å¯ç”¨KV cacheä¼˜åŒ–

2. **æ¨ç†ä¼˜åŒ–**:
   - ä½¿ç”¨ Flash Attention 2
   - å¯ç”¨ CUDA å›¾ä¼˜åŒ–
   - åˆç†è®¾ç½®batchå¤§å°

3. **å¹¶å‘ä¼˜åŒ–**:
   - è°ƒæ•´ `max_num_seqs` å‚æ•°
   - ä½¿ç”¨è¿æ¥æ± 
   - å¯ç”¨å¼‚æ­¥å¤„ç†

## ğŸ”„ æ›´æ–°å’Œç»´æŠ¤

### æ›´æ–°æ¨¡å‹
```bash
# åœæ­¢æœåŠ¡
sudo systemctl stop deepseek-vllm

# æ¸…é™¤æ¨¡å‹ç¼“å­˜
rm -rf model_cache/models--deepseek-ai--*

# é‡æ–°ä¸‹è½½æ¨¡å‹
./install.sh

# å¯åŠ¨æœåŠ¡
sudo systemctl start deepseek-vllm
```

### æ›´æ–°ä»£ç 
```bash
# å¤‡ä»½é…ç½®
cp DeepSeek-R1-Distill-Llama-8B-app.py DeepSeek-R1-Distill-Llama-8B-app.py.bak

# æ›´æ–°ä»£ç 
git pull

# é‡å¯æœåŠ¡
sudo systemctl restart deepseek-vllm
```

## ğŸ“ æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. æ—¥å¿—æ–‡ä»¶: `logs/vllm_app_*.log`
2. ç³»ç»Ÿæ—¥å¿—: `sudo journalctl -u deepseek-vllm`
3. GPUçŠ¶æ€: `nvidia-smi`
4. ç£ç›˜ç©ºé—´: `df -h`

å¸¸ç”¨è°ƒè¯•å‘½ä»¤ï¼š
```bash
# æŸ¥çœ‹è¯¦ç»†é”™è¯¯
python DeepSeek-R1-Distill-Llama-8B-app.py --log-level debug

# æµ‹è¯•CUDA
python -c "import torch; print(torch.cuda.is_available())"

# æµ‹è¯•vLLM
python -c "import vllm; print('vLLM imported successfully')"
``` 